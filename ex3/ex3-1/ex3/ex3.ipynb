{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00b9b14",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9717c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e5946",
   "metadata": {},
   "source": [
    "# Q3 - Implementing dynamic programming algorithms in the GridWorld domain.\n",
    "\n",
    "In this question, you are asked to implement the **value iteration** (See the pseudocode on page 83) and **policy iteration** (See the pseudocode on page 80). \n",
    "\n",
    "- The implementation of the GridWorld is given in the follow **GridWorld** class. Please use it to compute the dynamics and the reward\n",
    "- Please implement the value iteration and policy iteration in the following blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88988d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
    "# The GridWorld domain in Example 3.5\n",
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        # define the state space\n",
    "        self.state_space = [\n",
    "            [0, 0], [0, 1], [0, 2], [0, 3], [0, 4],\n",
    "            [1, 0], [1, 1], [1, 2], [1, 3], [1, 4],\n",
    "            [2, 0], [2, 1], [2, 2], [2, 3], [2, 4],\n",
    "            [3, 0], [3, 1], [3, 2], [3, 3], [3, 4],\n",
    "            [4, 0], [4, 1], [4, 2], [4, 3], [4, 4]\n",
    "        ]\n",
    "\n",
    "        # define special states\n",
    "        self.A, self.B = np.array([0, 1]), np.array([0, 3])\n",
    "        self.A_prime, self.B_prime = np.array([4, 1]), np.array([2, 3])\n",
    "\n",
    "        # define the action space\n",
    "        self.action_space = {\n",
    "            \"north\": [-1, 0],\n",
    "            \"south\": [1, 0],\n",
    "            \"west\": [0, -1],\n",
    "            \"east\": [0, 1]\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, s, a) -> (list, float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (list): a list contains the position of the current state\n",
    "            a (str): name of the action\n",
    "        \"\"\"\n",
    "        # convert the state to numpy array\n",
    "        s_arr = np.array(s)\n",
    "        # convert the action to numpy array\n",
    "        a_arr = np.array(self.action_space[a])\n",
    "\n",
    "        # compute the next state and reward using the dynamics function\n",
    "        next_s, r = self.dynamics_func(s_arr, a_arr)\n",
    "\n",
    "        # return the next state and the reward\n",
    "        return next_s, r\n",
    "\n",
    "    def dynamics_func(self, s_arr, a_arr) -> (list, float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s_arr (numpy.array): numpy array contains the position of the current state\n",
    "            a_arr (numpy.array): numpy array contains the change of the current state\n",
    "        \"\"\"\n",
    "        # check for special states A and B\n",
    "        # From state A, all four actions yield a reward of +10 and take the agent to A_prime\n",
    "        if np.array_equal(s_arr, self.A):\n",
    "            return self.A_prime.tolist(), 10.0\n",
    "\n",
    "        # From state B, all actions yield a reward of +5 and take the agent to B prime.\n",
    "        if np.array_equal(s_arr, self.B):\n",
    "            return self.B_prime.tolist(), 5.0\n",
    "\n",
    "        # check for normal states\n",
    "        # compute the next state position and reward\n",
    "        next_s = s_arr + a_arr\n",
    "        if next_s.tolist() not in self.state_space:\n",
    "            # Actions that would take the agent off the grid leave its location unchanged, but also result in a reward\n",
    "            # of -1\n",
    "            return s_arr.tolist(), -1.0\n",
    "        else:\n",
    "            # Other actions result in a reward of 0\n",
    "            return next_s.tolist(), 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ef860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
    "# Function to print the optimal state value\n",
    "def print_optimal_state_value(s_v):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        s_v (numpy.array): a 2-D numpy array contains the optimal state values with size 5 x 5\n",
    "    \"\"\"\n",
    "    print(\"=============================\")\n",
    "    print(\"==  Optimal State Value    ==\")\n",
    "    print(\"=============================\")  \n",
    "    print(s_v.round(decimals=1))\n",
    "    print(\"=============================\")\n",
    "\n",
    "# Function to print the optimal policy \n",
    "def print_optimal_policy(s_v, env, ga):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        s_v (numpy.array): a 2-D numpy array contains the optimal state value with size 5 x 5\n",
    "        env (env): the grid-world environment\n",
    "        ga (float): gamma \n",
    "    \"\"\"\n",
    "    print(\"=============================\")\n",
    "    print(\"==     Optimal Policy      ==\")\n",
    "    print(\"=============================\")\n",
    "    action_names = list(env.action_space.keys())\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            q_v = []\n",
    "            for a in env.action_space.keys():\n",
    "                next_s, r = env.step([i, j], a)\n",
    "                q_v.append(r + ga * s_v[next_s[0], next_s[1]])\n",
    "            q_v = np.array(q_v)\n",
    "\n",
    "            actions = np.where(q_v == q_v.max())[0]\n",
    "            actions = [action_names[a] for a in actions]\n",
    "\n",
    "            print(f\"{[i, j]} = {actions}\")\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448cf882",
   "metadata": {},
   "source": [
    "## Q3 - (a): Implement value iteration. Please complete the implementation of the value iteration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3aa8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We provide the scaffolding code of running the value iteration \n",
    "# Please implement the value iteration algorithm below \"\"\"CODE HERE\"\"\"\n",
    "def run_value_iteration(env, threshold, gamma):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        env: the grid-world environment, we use it to compute:\n",
    "            - the next state: s'\n",
    "            - the transition probability: p(s'|s,a)\n",
    "            - the reward : r\n",
    "        threshold: threshold determining the estimation threshold\n",
    "        gamma: the discounted factor\n",
    "        \n",
    "        Note: we use the vanilla implementation, where we maintain two separate numpy arrays to store the\n",
    "              state value and the updated state value. \n",
    "    \"\"\"\n",
    "    # initialize the state value to be 0\n",
    "    state_value = np.zeros((5, 5))\n",
    "    \n",
    "    # iteration counter\n",
    "    iter_counter = 0\n",
    "\n",
    "    # loop forever\n",
    "    while True:\n",
    "        # Logic: assuming the value iteration should be terminated for the current iteration\n",
    "        # unless there exists one state whose value estimation error > threshold. i.e. (abs(new_v - old_v) > threshold)\n",
    "        is_terminal = True\n",
    "\n",
    "        # save the new state value\n",
    "        new_state_value = np.zeros_like(state_value)\n",
    "\n",
    "        # loop all states \n",
    "        # each state is the position of the agent in the grid. e.g., [i, j]\n",
    "        # where i, j in [0, 4]\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                \"\"\" CODE HERE \"\"\"\n",
    "                # obtain the current state value estimation\n",
    "                old_v = None\n",
    "                \n",
    "                # compute the updated state value V(s) using equation 4.10.\n",
    "                # note that, \n",
    "                # 1. Use p(s'|s, a) rather than p(s', r|s, a).\n",
    "                # 2. The environment is deterministic. In other words, there is only one\n",
    "                #    possible s' and r given s and a.\n",
    "                new_v = None\n",
    "                \n",
    "                # check the termination\n",
    "                # set is_terminal = False if |new_v - old_v| > threshold\n",
    "                \n",
    "                \n",
    "                \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "                # store the updated value in the new_state_value\n",
    "                new_state_value[i, j] = new_value\n",
    "\n",
    "        # update the current state value with the updated values\n",
    "        state_value = new_state_value.copy()\n",
    "        \n",
    "        # terminate the loop \n",
    "        if is_terminal:\n",
    "            break\n",
    "\n",
    "    return state_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DO NOT CHANGE: it is used to run the value iteration above\"\"\"\n",
    "# run value iteration (DO NOT CHANGE)\n",
    "# create the envrionment\n",
    "my_grid = GridWorld()\n",
    "my_grid.reset()\n",
    "\n",
    "# threshold determining the accuracy of the estimation\n",
    "threshold = 1e-3\n",
    "\n",
    "# discounted factor\n",
    "gamma = 0.8\n",
    "\n",
    "# run the value iteration\n",
    "state_value = run_value_iteration(my_grid, threshold, gamma)\n",
    "\n",
    "# print the optimal state value\n",
    "print_optimal_state_value(state_value)\n",
    "\n",
    "# print the optimal policy\n",
    "print_optimal_policy(state_value, my_grid, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1f397",
   "metadata": {},
   "source": [
    "## Q3 - (b): Implement policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the scaffolding code of implement the policy evaluation\n",
    "# Please implement the value iteration algorithm below \"\"\"CODE HERE\"\"\"\n",
    "def policy_evaluation(env, policy, threshold, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the grid-world environment, we use it to compute:\n",
    "            - the next state: s'\n",
    "            - the transition probability: p(s'|s,a)\n",
    "            - the reward : r\n",
    "        policy (numpy.array): a 2-D numpy array stores the action to take at each location.\n",
    "        threshold (float): threshold determining the estimation threshold\n",
    "        gamma (float): the discounted factor\n",
    "        \n",
    "        Note: we use the vanilla implementation, where we maintain two separate numpy arrays to store the\n",
    "              state value and the updated state value. \n",
    "    \"\"\"\n",
    "    # initialize the state values\n",
    "    state_value = np.zeros((5, 5))\n",
    "\n",
    "    # start evaluate the current policy\n",
    "    while True:\n",
    "        # set terminal flag similar to the value iteration flag.\n",
    "        is_terminal = True\n",
    "\n",
    "        # new state value\n",
    "        new_state_value = np.zeros_like(state_value)\n",
    "\n",
    "        # loop all states\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                \"\"\" CODE HERE \"\"\"\n",
    "                # store the old state value\n",
    "                old_v = None\n",
    "                \n",
    "                # update the state value using the equation 4.5\n",
    "                # Hints: how many next state are there given a deterministic policy and \n",
    "                # a deterministic environment.   \n",
    "                new_v = None\n",
    "\n",
    "                # check termination\n",
    "                # set is_terminal = False if |new_v - old_v| > threshold\n",
    "                    \n",
    "                \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
    "                # store the updated state value for state [i, j]\n",
    "                new_state_value[i, j] = new_v\n",
    "\n",
    "        # update state value\n",
    "        state_value = new_state_value.copy()\n",
    "\n",
    "        # check termination\n",
    "        if is_terminal:\n",
    "            break\n",
    "\n",
    "    return state_value\n",
    "\n",
    "\n",
    "#################################\n",
    "# Implement policy improvement\n",
    "################################\n",
    "def policy_improvement(env, policy, state_value, gamma):\n",
    "    # set the policy improvement flag\n",
    "    policy_stable = True\n",
    "    \n",
    "    # loop all states\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            \"\"\" CODE HERE \"\"\"\n",
    "            # store the old action\n",
    "            old_a = None\n",
    "\n",
    "            # compute a new greedy action based on the latest state value\n",
    "            new_a = None\n",
    "            \n",
    "            # check if the policy is stable\n",
    "             \n",
    "            \"\"\" DO NOT CHANGE BELOW\"\"\"\n",
    "            # update the policy with the new greedy policy\n",
    "            policy[i, j] = new_a\n",
    "\n",
    "    return policy.astype(int), policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DO NOT CHANGE: it is used to run the policy iteration above\"\"\"\n",
    "# run policy iteration (DO NOT CHANGE)\n",
    "my_grid = GridWorld()\n",
    "my_grid.reset()\n",
    "\n",
    "# threshold and gamma\n",
    "threshold, gamma = 1e-3, 0.8\n",
    "\n",
    "# initialize a policy\n",
    "policy = np.random.randint(low=0, high=4, size=(5, 5))\n",
    "\n",
    "# run policy iteration\n",
    "while True:\n",
    "    # policy evaluation\n",
    "    state_value = policy_evaluation(my_grid, policy, threshold, gamma)\n",
    "\n",
    "    # policy improvement\n",
    "    policy, policy_stable = policy_improvement(my_grid, policy, state_value, gamma)\n",
    "\n",
    "    # check if policy is stable\n",
    "    if policy_stable:\n",
    "        break\n",
    "\n",
    "# print the optimal state value\n",
    "print_optimal_state_value(state_value)\n",
    "\n",
    "# print the optimal policy\n",
    "print_optimal_policy(state_value, my_grid, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052fc7c-3299-40dd-b49a-9622ee2844e8",
   "metadata": {},
   "source": [
    "# Q5 - Gamber's Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaaa02f-c5f0-4ac0-be57-888f2d86e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def states(target=100):\n",
    "    for s in range(1, target):\n",
    "        yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168230ca-5d74-4920-b99e-a3a9f900f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(action, state, V, ph, gamma=1, target=100):\n",
    "    action_return = 0\n",
    "\n",
    "    # success\n",
    "    sp = min(state + action, target)\n",
    "    action_return += ph * ((sp == target) + gamma * V[sp])\n",
    "\n",
    "    # fail\n",
    "    sp = max(state - action, 0)\n",
    "    action_return += (1 - ph) * (gamma * V[sp])\n",
    "\n",
    "    return action_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52db30f-32e3-4685-95ce-a82ee811d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration(target=100, ph=0.25):\n",
    "    # Initialize value function V with zeros for all states (0 to target)\n",
    "    V = np.zeros(target + 1)\n",
    "    \n",
    "    # Initialize policy array pi with undefined values\n",
    "    pi = np.empty(target + 1, dtype=int)\n",
    "    \n",
    "    # Set terminal states to 0 (no action required)\n",
    "    pi[0] = 0\n",
    "    pi[target] = 0\n",
    "\n",
    "    # Convergence threshold to determine when value iteration stops\n",
    "    theta = 1e-9\n",
    "    \n",
    "    # Initialize delta to track the maximum change in value updates\n",
    "    delta = 1\n",
    "    \n",
    "    # Counter for tracking iterations\n",
    "    iteration = 0\n",
    "\n",
    "    # List to store history of value functions for analysis\n",
    "    value_function_history = []\n",
    "\n",
    "    # Iterate until the change in value function is smaller than the threshold\n",
    "    while delta > theta:\n",
    "        iteration += 1  # Increment iteration count\n",
    "        delta = 0  # Reset delta for this iteration\n",
    "        \n",
    "        # Store the current value function to track progress\n",
    "        value_function_history.append(V.copy())\n",
    "\n",
    "        \"\"\" CODE HERE \"\"\"\n",
    "        # Loop through all valid states (excluding terminal states)\n",
    "        for s in states(target):\n",
    "            # TODO: Store old value for convergence check\n",
    "            # TODO: Store returns for all possible actions\n",
    "            pass\n",
    "\n",
    "            # Consider all possible bet sizes (valid actions)\n",
    "            for a in range(1, min(s, target - s) + 1):\n",
    "                # TODO: Compute expected return for taking action 'a' in state 's'\n",
    "                pass\n",
    "\n",
    "            # Update the value function with the best possible action return\n",
    "            V[s] = max(action_returns)\n",
    "\n",
    "            # Update delta with the maximum absolute change in value function\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "\n",
    "    # Store the final value function in history after convergence\n",
    "    value_function_history.append(V.copy())\n",
    "\n",
    "    # Compute the optimal policy after convergence\n",
    "    for s in states(target):\n",
    "        action_returns = []  # Store returns for all possible actions\n",
    "\n",
    "        \"\"\" CODE HERE \"\"\"\n",
    "        # TODO: List of all possible bet sizes for the current state\n",
    "        actions = None\n",
    "\n",
    "        # TODO: Compute returns for each possible action\n",
    "        for a in actions:\n",
    "            pass\n",
    "\n",
    "        # Choose the action that maximizes the rounded expected return\n",
    "        pi[s] = actions[np.argmax(np.round(action_returns[1:], 5)) + 1]\n",
    "    \n",
    "    # Return the optimal policy and the history of value functions\n",
    "    return pi, value_function_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d847827-c549-41f1-ae9e-d1dae76772bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DO NOT CHANGE: plot function\"\"\"\n",
    "def plot(target, ph, pi, value_function_history):\n",
    "    plt.figure(figsize=(10, 20))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for sweep, state_value in enumerate(value_function_history[:10]):\n",
    "        plt.plot(state_value[1:100], label='sweep {}'.format(sweep), linewidth='0.9')\n",
    "\n",
    "    if len(value_function_history)>10:\n",
    "        plt.plot(value_function_history[-1][1:100], label='sweep {}'.format(len(value_function_history)-1))\n",
    "\n",
    "    plt.title('Ph={}'.format(ph))\n",
    "    plt.xlabel('Capital')\n",
    "    plt.ylabel('Value estimates')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(np.arange(target + 1), pi)\n",
    "    plt.xlabel('Capital')\n",
    "    plt.ylabel('Final policy (stake)')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af24253-0705-4795-93f0-6547c923ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pi, value_function_history = value_iteration(target=100, ph=0.25)\n",
    "plot(100, 0.25, pi, value_function_history)\n",
    "pi, value_function_history = value_iteration(target=100, ph=0.55)\n",
    "plot(100, 0.55, pi, value_function_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c44e0-1469-4650-ae29-df7e283150f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
