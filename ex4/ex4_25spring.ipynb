{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b1f1b8",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "- scipy: https://scipy.org/install/\n",
    "- gymnasium: https://github.com/Farama-Foundation/Gymnasium (**New package to install**)\n",
    "    - To install the base Gymnasium library, use **pip install gymnasium**\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d007d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729baf6-191d-4b40-9867-50ec8ce266d0",
   "metadata": {},
   "source": [
    "# Q2: Backjack \n",
    "\n",
    "Please note, since there is no scaffolding code for this question. Please make sure your implementation is well-tested and the comments are clear to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code for using the \"Backjack\" implementation from OpenAI\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env.reset()\n",
    "\n",
    "# iteracting with the environment for 10 time steps using a random policy\n",
    "for t in range(10): \n",
    "    # sample an random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # interact with the envrionment\n",
    "    next_s, reward, done, info, _ = env.step(action)\n",
    "    \n",
    "    # print info\n",
    "    print(f\"t = {t}, next_state = {next_s}, reward = {reward}, done = {done}\")\n",
    "    \n",
    "    # check termination\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09fa47",
   "metadata": {},
   "source": [
    "## (a): Implement first-visit Monte-Carlo policy evaluation (See pseudocode in 5.1). \n",
    "\n",
    "Please read Example 5.1 and reproduce Figure 1. The policy to be evaluated is the \"sticks only on 20 or 21\". Please only reproduce the plots after 500, 000 episodes (Usable ace and No usable ace). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5acd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CODE HERE YOUR IMPLEMENTATION for Q2-(a) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d2bec",
   "metadata": {},
   "source": [
    "## (b): Implement first-visit Monte-Carlo control with exploring starts (Monte-Carlo ES in 5.3). \n",
    "\n",
    "Please read the Example 5.1 and reproduce the Figure 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b982cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CODE HERE YOUR IMPLEMENTATION for Q2-(b) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ac36f",
   "metadata": {},
   "source": [
    "# Q3: Four Rooms, re-visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DO NOT CHANGE\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # define the four room as a 2-D array for easy state space reference and visualization\n",
    "        # 0 represents an empty cell; 1 represents a wall cell\n",
    "        self.four_room_space = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # find the positions for all empty cells\n",
    "        # note that: the origin for a 2-D numpy array is located at top-left while the origin for the FourRooms is at\n",
    "        # the bottom-left. The following codes performs the re-projection.\n",
    "        empty_cells = np.where(self.four_room_space == 0.0)\n",
    "        self.state_space = [[col, 10 - row] for row, col in zip(empty_cells[0], empty_cells[1])]\n",
    "\n",
    "        # define the action space\n",
    "        self.action_space = {'LEFT': np.array([-1, 0]),\n",
    "                             'RIGHT': np.array([1, 0]),\n",
    "                             'DOWN': np.array([0, -1]),\n",
    "                             'UP': np.array([0, 1])}\n",
    "\n",
    "        # define the start state\n",
    "        self.start_state = [0, 0]\n",
    "\n",
    "        # define the goal state\n",
    "        self.goal_state = [10, 10]\n",
    "\n",
    "        # maximal time steps\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # track the time step\n",
    "        self.t = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent's state to the start state [0, 0]\n",
    "        Return both the start state and reward\n",
    "        \"\"\"\n",
    "        # reset the agent state to be [0, 0]\n",
    "        state = self.start_state\n",
    "        # reset the reward to be 0\n",
    "        reward = 0\n",
    "        # reset the termination flag\n",
    "        done = False\n",
    "        # reset the time step tracker\n",
    "        self.t = 0\n",
    "        return state, reward, done\n",
    "\n",
    "    def step(self, state, act):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
    "            act: a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
    "        Output args:\n",
    "            next_state: a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
    "            reward: an integer. it can be either 0 or 1.\n",
    "        \"\"\"\n",
    "        # Increase the time step\n",
    "        self.t += 1\n",
    "\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if act == \"LEFT\" or act == \"RIGHT\":\n",
    "                act = np.random.choice([\"UP\", \"DOWN\"], 1)[0]\n",
    "            else:\n",
    "                act = np.random.choice([\"RIGHT\", \"LEFT\"], 1)[0]\n",
    "\n",
    "        # Compute the next state\n",
    "        next_state = self.take_action(state, act)\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = 1.0 if next_state == [10, 10] else 0.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 1, done = True\n",
    "        # If the time steps reaches the maximal number, reward = 0, done = True.\n",
    "        if next_state == [10, 10] or self.t == self.max_time_steps:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def take_action(self, state, act):\n",
    "        \"\"\"\n",
    "        Input args:\n",
    "            state (list): a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
    "            act (string): a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
    "        Output args:\n",
    "            next_state (list): a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        next_state = state + self.action_space[act]\n",
    "        return next_state.tolist() if next_state.tolist() in self.state_space else state.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b9ee4",
   "metadata": {},
   "source": [
    "## (a): Implement first-visit Monte-Carlo control with a epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3476408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CODE HERE YOUR IMPLEMENTATION for Q3-(a) \"\"\"\n",
    "# Implementation of the one-policy first-vist Monte-Carlo control (for Îµ-soft policies) here.\n",
    "# To debug your code, you can use run_num = 5 and episode_num = 1e3\n",
    "# To report the finial results, please use run_num = 10 and episode_num = 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd96e7",
   "metadata": {},
   "source": [
    "## Q5: Racktrack\n",
    "\n",
    "Please note, we provde you with the implementation of the two Racetrack domains in the Figure 5.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91609432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Racetrack domain 1\n",
    "racetrack_v1_arr = np.array([\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "racetrack_v2_arr = np.array([\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c019eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, arr = plt.subplots(1, 2)\n",
    "arr[0].set_title(\"Racetrack v1\")\n",
    "arr[0].imshow(racetrack_v1_arr)\n",
    "arr[1].set_title(\"Racetrack v2\")\n",
    "arr[1].imshow(racetrack_v2_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of the Racetrack environment. Do not change\"\"\"\n",
    "class Racetrack(object):\n",
    "    def __init__(self, version):\n",
    "        # Load the pre-defined the domain having the following representation\n",
    "        #   - 1: track cell\n",
    "        #   - 0: empty cell\n",
    "        #   - 2: empty cell on the start line\n",
    "        #   - 3: empty cell on the finish line\n",
    "        if version == \"v1\":\n",
    "            self.domain_arr = racetrack_v1_arr.copy()\n",
    "        else:\n",
    "            self.domain_arr = racetrack_v2_arr.copy()\n",
    "\n",
    "        # domain size\n",
    "        self.height, self.width = self.domain_arr.shape\n",
    "\n",
    "        # State space consists of:\n",
    "        # Agent location\n",
    "        self.empty_cell_locs = self.render_cell_locations(val=0.0)\n",
    "        self.track_cell_locs = self.render_cell_locations(val=1.0)\n",
    "        self.start_cell_locs = self.render_cell_locations(val=2.0)\n",
    "        self.finish_cell_locs = self.render_cell_locations(val=3.0)\n",
    "\n",
    "        # Action space\n",
    "        self.action_space = [[-1, -1], [-1, 0], [-1, 1],\n",
    "                             [0, -1], [0, 0], [0, 1],\n",
    "                             [1, -1], [1, 0], [1, 1]]\n",
    "\n",
    "        # construct the state space\n",
    "        self.state_space = []\n",
    "        for loc in self.start_cell_locs + self.empty_cell_locs + self.finish_cell_locs:\n",
    "            for i in range(5):\n",
    "                for j in range(5):\n",
    "                    self.state_space.append(loc + [i, j])\n",
    "\n",
    "        # track the agent's location\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.t = None\n",
    "\n",
    "    def reset(self):\n",
    "        # randomly select one cell from the start line\n",
    "        start_loc = random.sample(self.start_cell_locs, 1)[0]\n",
    "        # reset the velocity to be zero for both x and y directions\n",
    "        start_vel = [0, 0]\n",
    "        # the state is a combination of location and velocity\n",
    "        # for example: [loc_x, loc_y, vel_x, vel_y]\n",
    "        state = start_loc + start_vel\n",
    "        # reward\n",
    "        reward = None\n",
    "        # done\n",
    "        done = False\n",
    "        # track agent's location\n",
    "        self.state = tuple(state)\n",
    "        self.t = 0\n",
    "        return state, reward, done\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (list): a list variable consists of agent's location + agent's current velocity. e.g., [x, y, v_x, v_y]\n",
    "            action (list): a list variable consists of agent's velocity increments. e.g., [increments_v_x, increments_v_y]\n",
    "        \"\"\"\n",
    "        # reward is -1 for every time step until the agent passes the finish line\n",
    "        reward = -1\n",
    "        self.t += 1\n",
    "        \n",
    "        # with the probability = 0.1, set action = [0, 0]\n",
    "        if np.random.random() < 0.1:\n",
    "            action = [0, 0]\n",
    "\n",
    "        # update the velocity components\n",
    "        # note that, both velocity is discrete and constraint within [0, 4]\n",
    "        next_vel_x = np.clip(state[2] + action[0], a_min=0, a_max=4)\n",
    "        next_vel_y = np.clip(state[3] + action[1], a_min=0, a_max=4)\n",
    "        next_state_vel = [next_vel_x, next_vel_y]\n",
    "\n",
    "        # only the cells on the start line can have both 0 velocities\n",
    "        if next_state_vel == [0, 0]:\n",
    "            if state not in self.start_cell_locs:\n",
    "                # non-zero for velocities\n",
    "                if state[2] == 0 and state[3] != 0:\n",
    "                    next_state_vel = [0, 1]\n",
    "                if state[2] != 0 and state[3] == 0:\n",
    "                    next_state_vel = [1, 0]\n",
    "                if state[2] != 0 and state[3] != 0:\n",
    "                    non_zero_idx = random.sample([0, 1], 1)[0]\n",
    "                    next_state_vel[non_zero_idx] = 1\n",
    "\n",
    "        # update the next state location based on the updated velocities\n",
    "        next_state_loc = [np.clip(state[0] + next_state_vel[0], a_min=0, a_max=self.width-1),\n",
    "                          np.clip(state[1] + next_state_vel[1], a_min=0, a_max=self.height-1)]\n",
    "\n",
    "        # check whether the agent hits the track\n",
    "        if next_state_loc in self.track_cell_locs:\n",
    "            # move back to the start line\n",
    "            next_state_loc = random.sample(self.start_cell_locs, 1)[0]\n",
    "            # reduce velocity to be 0s\n",
    "            next_state_vel = [0, 0]\n",
    "            # episode continue\n",
    "            done = False\n",
    "            # next state\n",
    "            next_state = next_state_loc + next_state_vel\n",
    "            return next_state, reward, done\n",
    "\n",
    "        # check whether the agent pass the finish line\n",
    "        if next_state_loc in self.finish_cell_locs:\n",
    "            next_state = next_state_loc + next_state_vel\n",
    "            done = True\n",
    "            return next_state, 0, done\n",
    "\n",
    "        # otherwise combine the next state\n",
    "        next_state = next_state_loc + next_state_vel\n",
    "        # termination\n",
    "        done = False\n",
    "\n",
    "        # track the agent's state\n",
    "        self.state = tuple(next_state)\n",
    "        self.action = action\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render_cell_locations(self, val):\n",
    "        row_loc_indices, col_loc_indices = np.where(self.domain_arr == val)\n",
    "        cell_locations = [[c, (self.height-1) - r] for r, c in zip(row_loc_indices, col_loc_indices)]\n",
    "        return cell_locations\n",
    "\n",
    "    def render(self):\n",
    "        plt.clf()\n",
    "        plt.title(f\"s = {self.state}, a = {self.action}\")\n",
    "        plot_arr = self.domain_arr.copy()\n",
    "        plot_arr[(self.height - 1) - self.state[1], self.state[0]] = 4\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db784d5",
   "metadata": {},
   "source": [
    "## (a): Implement first-visit Monte-Carlo control with a epsilon-greedy policy (epsilon = 0.1)\n",
    "    - Plot the learning curves for the two tracks.\n",
    "    - Use running trials number = 10 and episodes number = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26355b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CODE HERE YOUR IMPLEMENTATION for Q5-(a) \"\"\"\n",
    "# Implement here and use the plotting function above to plot the learning curve "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb24781",
   "metadata": {},
   "source": [
    "## (b): Implement off-policy Monte-Carlo control. Please specify what behavior policy you are using.\n",
    "    - Plot the learning curves for the two tracks\n",
    "    - Using running trials number = 10 and episodes number = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6889433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CODE HERE YOUR IMPLEMENTATION for Q5-(b) \"\"\"\n",
    "# Implement here and use the plotting function above to plot the learning curve "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
